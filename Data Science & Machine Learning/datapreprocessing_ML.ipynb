{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4433798,"sourceType":"datasetVersion","datasetId":2596646}],"dockerImageVersionId":30301,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div class=\"alert alert-block alert-success\" dir=\"rtl\" style=\"text-align: center;\"><strong><span style=\"font-size: 20pt\">Data PreProcessing <br /></span></strong></div>","metadata":{}},{"cell_type":"markdown","source":"# <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/08/584692017-PGHD.jpg\" />\n","metadata":{}},{"cell_type":"markdown","source":"# <font color='blue'>Summary:</font>\n1- Acquire the dataset\n\n2- Import all the crucial libraries\n\n3- Import the dataset\n\n4- Identifying and handling the missing values\n\n5- Encoding the categorical data\n\n6- Splitting the dataset\n\n7- Feature scaling\n\n<font color = \"purple\">Data preprocessing in Machine Learning is a crucial step that helps enhance the quality of data to promote the extraction of meaningful insights from the data. Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models. In simple words, data preprocessing in Machine Learning is a data mining technique that transforms raw data into an understandable and readable format. </font>","metadata":{}},{"cell_type":"markdown","source":"# <font color='deepskyblue'>Why Data Preprocessing in Machine Learning?</font>\nWhen it comes to creating a Machine Learning model, data preprocessing is the first step marking the initiation of the process. Typically, real-world data is incomplete, inconsistent, inaccurate (contains errors or outliers), and often lacks specific attribute values/trends. This is where data preprocessing enters the scenario – it helps to clean, format, and organize the raw data, thereby making it ready-to-go for Machine Learning models. Let’s explore various steps of data preprocessing in machine learning.","metadata":{}},{"cell_type":"markdown","source":"# <font color='darkcyan'>1- Acquire the dataset</font>\nAcquiring the dataset is the first step in data preprocessing in machine learning. To build and develop Machine Learning models, you must first acquire the relevant dataset. This dataset will be comprised of data gathered from multiple and disparate sources which are then combined in a proper format to form a dataset. Dataset formats differ according to use cases. For instance, a business dataset will be entirely different from a medical dataset. While a business dataset will contain relevant industry and business data, a medical dataset will include healthcare-related data.","metadata":{}},{"cell_type":"markdown","source":"# <font color='lightseagreen'>2- Import all the crucial libraries</font>\nThe predefined Python libraries can perform specific data preprocessing jobs. Importing all the crucial libraries is the second step in data preprocessing in machine learning. ","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:01.472613Z","iopub.execute_input":"2024-05-03T00:08:01.473860Z","iopub.status.idle":"2024-05-03T00:08:03.148014Z","shell.execute_reply.started":"2024-05-03T00:08:01.473712Z","shell.execute_reply":"2024-05-03T00:08:03.146449Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# <font color='cadetblue'>3- Import the dataset</font>\nIn this step, you need to import the dataset/s that you have gathered for the ML project at hand. Importing the dataset is one of the important steps in data preprocessing in machine learning.\n","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv(r\"../input/preprocessingdataset/Data.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.151028Z","iopub.execute_input":"2024-05-03T00:08:03.152267Z","iopub.status.idle":"2024-05-03T00:08:03.174149Z","shell.execute_reply.started":"2024-05-03T00:08:03.152198Z","shell.execute_reply":"2024-05-03T00:08:03.173047Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.176736Z","iopub.execute_input":"2024-05-03T00:08:03.178014Z","iopub.status.idle":"2024-05-03T00:08:03.184573Z","shell.execute_reply.started":"2024-05-03T00:08:03.177961Z","shell.execute_reply":"2024-05-03T00:08:03.182930Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.188536Z","iopub.execute_input":"2024-05-03T00:08:03.189229Z","iopub.status.idle":"2024-05-03T00:08:03.226061Z","shell.execute_reply.started":"2024-05-03T00:08:03.189166Z","shell.execute_reply":"2024-05-03T00:08:03.224516Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Country   Age   Salary Purchased\n0   France  44.0  72000.0        No\n1    Spain  27.0  48000.0       Yes\n2  Germany  30.0  54000.0        No\n3    Spain  38.0  61000.0        No\n4  Germany  40.0      NaN       Yes\n5   France  35.0  58000.0       Yes\n6    Spain   NaN  52000.0        No\n7   France  48.0  79000.0       Yes\n8  Germany  50.0  83000.0        No\n9   France  37.0  67000.0       Yes","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Age</th>\n      <th>Salary</th>\n      <th>Purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>France</td>\n      <td>44.0</td>\n      <td>72000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Spain</td>\n      <td>27.0</td>\n      <td>48000.0</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Germany</td>\n      <td>30.0</td>\n      <td>54000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Spain</td>\n      <td>38.0</td>\n      <td>61000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Germany</td>\n      <td>40.0</td>\n      <td>NaN</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>France</td>\n      <td>35.0</td>\n      <td>58000.0</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Spain</td>\n      <td>NaN</td>\n      <td>52000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>France</td>\n      <td>48.0</td>\n      <td>79000.0</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Germany</td>\n      <td>50.0</td>\n      <td>83000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>France</td>\n      <td>37.0</td>\n      <td>67000.0</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.228556Z","iopub.execute_input":"2024-05-03T00:08:03.228957Z","iopub.status.idle":"2024-05-03T00:08:03.243416Z","shell.execute_reply.started":"2024-05-03T00:08:03.228922Z","shell.execute_reply":"2024-05-03T00:08:03.241192Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.245772Z","iopub.execute_input":"2024-05-03T00:08:03.246452Z","iopub.status.idle":"2024-05-03T00:08:03.261343Z","shell.execute_reply.started":"2024-05-03T00:08:03.246396Z","shell.execute_reply":"2024-05-03T00:08:03.258981Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 nan]\n ['France' 35.0 58000.0]\n ['Spain' nan 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(y)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.263710Z","iopub.execute_input":"2024-05-03T00:08:03.264267Z","iopub.status.idle":"2024-05-03T00:08:03.274821Z","shell.execute_reply.started":"2024-05-03T00:08:03.264223Z","shell.execute_reply":"2024-05-03T00:08:03.272645Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font color='dodgerblue'>4- Identifying and handling the missing values</font>\nIn data preprocessing, it is pivotal to identify and correctly handle the missing values, failing to do this, you might draw inaccurate and faulty conclusions and inferences from the data. Needless to say, this will hamper your ML project. \n\nsome typical reasons why data is missing:\n\nA. User forgot to fill in a field.\n\nB. Data was lost while transferring manually from a legacy database.\n\nC. There was a programming error.\n\nD. Users chose not to fill out a field tied to their beliefs about how the results would be used or interpreted.\n\nBasically, there are two ways to handle missing data:\n\nDeleting a particular row – In this method, you remove a specific row that has a null value for a feature or a particular column where more than 75% of the values are missing. However, this method is not 100% efficient, and it is recommended that you use it only when the dataset has adequate samples. You must ensure that after deleting the data, there remains no addition of bias. \nCalculating the mean – This method is useful for features having numeric data like age, salary, year, etc. Here, you can calculate the mean, median, or mode of a particular feature or column or row that contains a missing value and replace the result for the missing value. This method can add variance to the dataset, and any loss of data can be efficiently negated. Hence, it yields better results compared to the first method (omission of rows/columns). Another way of approximation is through the deviation of neighbouring values. However, this works best for linear data.\n","metadata":{}},{"cell_type":"markdown","source":"# <img src=\"https://files.realpython.com/media/Pythons-None-Type-Null-in-Python_Watermarked.9d48d487f417.jpg\" width=\"70%\"/>\n","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.277360Z","iopub.execute_input":"2024-05-03T00:08:03.277959Z","iopub.status.idle":"2024-05-03T00:08:03.292674Z","shell.execute_reply.started":"2024-05-03T00:08:03.277918Z","shell.execute_reply":"2024-05-03T00:08:03.291408Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Country      0\nAge          1\nSalary       1\nPurchased    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# <font color='lime'>Solution 1 : Dropna</font>","metadata":{}},{"cell_type":"code","source":"df1 = df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.294723Z","iopub.execute_input":"2024-05-03T00:08:03.295166Z","iopub.status.idle":"2024-05-03T00:08:03.309059Z","shell.execute_reply.started":"2024-05-03T00:08:03.295128Z","shell.execute_reply":"2024-05-03T00:08:03.307489Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# summarize the shape of the raw data\nprint(\"Before:\",df1.shape)\n\n# drop rows with missing values\ndf1.dropna(inplace=True)\n\n# summarize the shape of the data with missing rows removed\nprint(\"After:\",df1.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.316465Z","iopub.execute_input":"2024-05-03T00:08:03.317102Z","iopub.status.idle":"2024-05-03T00:08:03.334908Z","shell.execute_reply.started":"2024-05-03T00:08:03.317052Z","shell.execute_reply":"2024-05-03T00:08:03.332462Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Before: (10, 4)\nAfter: (8, 4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font color='chartreuse'>Solution 2 : Fillna</font>","metadata":{}},{"cell_type":"code","source":"df2 = df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.337019Z","iopub.execute_input":"2024-05-03T00:08:03.337577Z","iopub.status.idle":"2024-05-03T00:08:03.348596Z","shell.execute_reply.started":"2024-05-03T00:08:03.337532Z","shell.execute_reply":"2024-05-03T00:08:03.347410Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.349937Z","iopub.execute_input":"2024-05-03T00:08:03.350456Z","iopub.status.idle":"2024-05-03T00:08:03.367298Z","shell.execute_reply.started":"2024-05-03T00:08:03.350407Z","shell.execute_reply":"2024-05-03T00:08:03.365693Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# fill missing values with mean column values\ndf2.fillna(df2.mean(), inplace=True)\n# count the number of NaN values in each column\nprint(df2.isnull().sum())\n\ndf2","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.369475Z","iopub.execute_input":"2024-05-03T00:08:03.370419Z","iopub.status.idle":"2024-05-03T00:08:03.405161Z","shell.execute_reply.started":"2024-05-03T00:08:03.370363Z","shell.execute_reply":"2024-05-03T00:08:03.403642Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Country      0\nAge          0\nSalary       0\nPurchased    0\ndtype: int64\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   Country        Age        Salary Purchased\n0   France  44.000000  72000.000000        No\n1    Spain  27.000000  48000.000000       Yes\n2  Germany  30.000000  54000.000000        No\n3    Spain  38.000000  61000.000000        No\n4  Germany  40.000000  63777.777778       Yes\n5   France  35.000000  58000.000000       Yes\n6    Spain  38.777778  52000.000000        No\n7   France  48.000000  79000.000000       Yes\n8  Germany  50.000000  83000.000000        No\n9   France  37.000000  67000.000000       Yes","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Age</th>\n      <th>Salary</th>\n      <th>Purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>France</td>\n      <td>44.000000</td>\n      <td>72000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Spain</td>\n      <td>27.000000</td>\n      <td>48000.000000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Germany</td>\n      <td>30.000000</td>\n      <td>54000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Spain</td>\n      <td>38.000000</td>\n      <td>61000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Germany</td>\n      <td>40.000000</td>\n      <td>63777.777778</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>France</td>\n      <td>35.000000</td>\n      <td>58000.000000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Spain</td>\n      <td>38.777778</td>\n      <td>52000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>France</td>\n      <td>48.000000</td>\n      <td>79000.000000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Germany</td>\n      <td>50.000000</td>\n      <td>83000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>France</td>\n      <td>37.000000</td>\n      <td>67000.000000</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# <font color='darkgreen'>Solution 3 : Scikit-Learn</font>\n# <img src=\"https://miro.medium.com/max/693/1*1ouD8HMkmJffNSAMfvBSkw.png\" width=\"50%\"/>\n","metadata":{}},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.407446Z","iopub.execute_input":"2024-05-03T00:08:03.407970Z","iopub.status.idle":"2024-05-03T00:08:03.418677Z","shell.execute_reply.started":"2024-05-03T00:08:03.407925Z","shell.execute_reply":"2024-05-03T00:08:03.417402Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([['France', 44.0, 72000.0],\n       ['Spain', 27.0, 48000.0],\n       ['Germany', 30.0, 54000.0],\n       ['Spain', 38.0, 61000.0],\n       ['Germany', 40.0, nan],\n       ['France', 35.0, 58000.0],\n       ['Spain', nan, 52000.0],\n       ['France', 48.0, 79000.0],\n       ['Germany', 50.0, 83000.0],\n       ['France', 37.0, 67000.0]], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:, 1:3])\nX[:, 1:3] = imputer.transform(X[:, 1:3])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.420236Z","iopub.execute_input":"2024-05-03T00:08:03.420762Z","iopub.status.idle":"2024-05-03T00:08:03.717657Z","shell.execute_reply.started":"2024-05-03T00:08:03.420709Z","shell.execute_reply":"2024-05-03T00:08:03.715968Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.722914Z","iopub.execute_input":"2024-05-03T00:08:03.723398Z","iopub.status.idle":"2024-05-03T00:08:03.731359Z","shell.execute_reply.started":"2024-05-03T00:08:03.723349Z","shell.execute_reply":"2024-05-03T00:08:03.730067Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 63777.77777777778]\n ['France' 35.0 58000.0]\n ['Spain' 38.77777777777778 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font color='royalblue'>5- Encoding the categorical data</font>\nCategorical data refers to the information that has specific categories within the dataset. In the dataset cited above, there are two categorical variables – country and purchased.\n\nMachine Learning models are primarily based on mathematical equations. Thus, you can intuitively understand that keeping the categorical data in the equation will cause certain issues since you would only need numbers in the equations.","metadata":{}},{"cell_type":"markdown","source":"# <font color='darkorchid'>Solution 1 : ColumnTransformer</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.732907Z","iopub.execute_input":"2024-05-03T00:08:03.733369Z","iopub.status.idle":"2024-05-03T00:08:03.764814Z","shell.execute_reply.started":"2024-05-03T00:08:03.733317Z","shell.execute_reply":"2024-05-03T00:08:03.762895Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.766971Z","iopub.execute_input":"2024-05-03T00:08:03.767466Z","iopub.status.idle":"2024-05-03T00:08:03.786427Z","shell.execute_reply.started":"2024-05-03T00:08:03.767426Z","shell.execute_reply":"2024-05-03T00:08:03.784647Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"   Country   Age   Salary Purchased\n0   France  44.0  72000.0        No\n1    Spain  27.0  48000.0       Yes\n2  Germany  30.0  54000.0        No\n3    Spain  38.0  61000.0        No\n4  Germany  40.0      NaN       Yes\n5   France  35.0  58000.0       Yes\n6    Spain   NaN  52000.0        No\n7   France  48.0  79000.0       Yes\n8  Germany  50.0  83000.0        No\n9   France  37.0  67000.0       Yes","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Age</th>\n      <th>Salary</th>\n      <th>Purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>France</td>\n      <td>44.0</td>\n      <td>72000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Spain</td>\n      <td>27.0</td>\n      <td>48000.0</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Germany</td>\n      <td>30.0</td>\n      <td>54000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Spain</td>\n      <td>38.0</td>\n      <td>61000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Germany</td>\n      <td>40.0</td>\n      <td>NaN</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>France</td>\n      <td>35.0</td>\n      <td>58000.0</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Spain</td>\n      <td>NaN</td>\n      <td>52000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>France</td>\n      <td>48.0</td>\n      <td>79000.0</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Germany</td>\n      <td>50.0</td>\n      <td>83000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>France</td>\n      <td>37.0</td>\n      <td>67000.0</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.788389Z","iopub.execute_input":"2024-05-03T00:08:03.788975Z","iopub.status.idle":"2024-05-03T00:08:03.798857Z","shell.execute_reply.started":"2024-05-03T00:08:03.788922Z","shell.execute_reply":"2024-05-03T00:08:03.797282Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[[1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [0.0 1.0 0.0 30.0 54000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 35.0 58000.0]\n [0.0 0.0 1.0 38.77777777777778 52000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font color='deeppink'>Solution 2 : Pd.get_dummies()</font>","metadata":{}},{"cell_type":"code","source":"df2","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.800504Z","iopub.execute_input":"2024-05-03T00:08:03.801209Z","iopub.status.idle":"2024-05-03T00:08:03.824117Z","shell.execute_reply.started":"2024-05-03T00:08:03.801169Z","shell.execute_reply":"2024-05-03T00:08:03.822648Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"   Country        Age        Salary Purchased\n0   France  44.000000  72000.000000        No\n1    Spain  27.000000  48000.000000       Yes\n2  Germany  30.000000  54000.000000        No\n3    Spain  38.000000  61000.000000        No\n4  Germany  40.000000  63777.777778       Yes\n5   France  35.000000  58000.000000       Yes\n6    Spain  38.777778  52000.000000        No\n7   France  48.000000  79000.000000       Yes\n8  Germany  50.000000  83000.000000        No\n9   France  37.000000  67000.000000       Yes","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Age</th>\n      <th>Salary</th>\n      <th>Purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>France</td>\n      <td>44.000000</td>\n      <td>72000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Spain</td>\n      <td>27.000000</td>\n      <td>48000.000000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Germany</td>\n      <td>30.000000</td>\n      <td>54000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Spain</td>\n      <td>38.000000</td>\n      <td>61000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Germany</td>\n      <td>40.000000</td>\n      <td>63777.777778</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>France</td>\n      <td>35.000000</td>\n      <td>58000.000000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Spain</td>\n      <td>38.777778</td>\n      <td>52000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>France</td>\n      <td>48.000000</td>\n      <td>79000.000000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Germany</td>\n      <td>50.000000</td>\n      <td>83000.000000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>France</td>\n      <td>37.000000</td>\n      <td>67000.000000</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pd.get_dummies(df2)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.826663Z","iopub.execute_input":"2024-05-03T00:08:03.827072Z","iopub.status.idle":"2024-05-03T00:08:03.860178Z","shell.execute_reply.started":"2024-05-03T00:08:03.827036Z","shell.execute_reply":"2024-05-03T00:08:03.858529Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"         Age        Salary  Country_France  Country_Germany  Country_Spain  \\\n0  44.000000  72000.000000               1                0              0   \n1  27.000000  48000.000000               0                0              1   \n2  30.000000  54000.000000               0                1              0   \n3  38.000000  61000.000000               0                0              1   \n4  40.000000  63777.777778               0                1              0   \n5  35.000000  58000.000000               1                0              0   \n6  38.777778  52000.000000               0                0              1   \n7  48.000000  79000.000000               1                0              0   \n8  50.000000  83000.000000               0                1              0   \n9  37.000000  67000.000000               1                0              0   \n\n   Purchased_No  Purchased_Yes  \n0             1              0  \n1             0              1  \n2             1              0  \n3             1              0  \n4             0              1  \n5             0              1  \n6             1              0  \n7             0              1  \n8             1              0  \n9             0              1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Salary</th>\n      <th>Country_France</th>\n      <th>Country_Germany</th>\n      <th>Country_Spain</th>\n      <th>Purchased_No</th>\n      <th>Purchased_Yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44.000000</td>\n      <td>72000.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27.000000</td>\n      <td>48000.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30.000000</td>\n      <td>54000.000000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38.000000</td>\n      <td>61000.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40.000000</td>\n      <td>63777.777778</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>35.000000</td>\n      <td>58000.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>38.777778</td>\n      <td>52000.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>48.000000</td>\n      <td>79000.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>50.000000</td>\n      <td>83000.000000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>37.000000</td>\n      <td>67000.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# <font color='magenta'>Solution 3 : LabelEncoder</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.862252Z","iopub.execute_input":"2024-05-03T00:08:03.862857Z","iopub.status.idle":"2024-05-03T00:08:03.870725Z","shell.execute_reply.started":"2024-05-03T00:08:03.862804Z","shell.execute_reply":"2024-05-03T00:08:03.868882Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(y)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.873878Z","iopub.execute_input":"2024-05-03T00:08:03.874537Z","iopub.status.idle":"2024-05-03T00:08:03.886504Z","shell.execute_reply.started":"2024-05-03T00:08:03.874492Z","shell.execute_reply":"2024-05-03T00:08:03.885261Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[0 1 0 0 1 1 0 1 0 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font color='aqua'>6- Splitting the dataset</font>\nSplitting the dataset is the next step in data preprocessing in machine learning. Every dataset for Machine Learning model must be split into two separate sets – training set and test set. ","metadata":{}},{"cell_type":"markdown","source":"# <img src=\"https://files.realpython.com/media/Splitting-Datasets-With-sklearns-train_test_split_Watermarked.13dcac93b15d.jpg\" width=\"75%\"/>\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.888167Z","iopub.execute_input":"2024-05-03T00:08:03.888611Z","iopub.status.idle":"2024-05-03T00:08:03.900044Z","shell.execute_reply.started":"2024-05-03T00:08:03.888574Z","shell.execute_reply":"2024-05-03T00:08:03.898861Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.901690Z","iopub.execute_input":"2024-05-03T00:08:03.903577Z","iopub.status.idle":"2024-05-03T00:08:03.914131Z","shell.execute_reply.started":"2024-05-03T00:08:03.903519Z","shell.execute_reply":"2024-05-03T00:08:03.912925Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[[0.0 0.0 1.0 38.77777777777778 52000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 35.0 58000.0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.915579Z","iopub.execute_input":"2024-05-03T00:08:03.916815Z","iopub.status.idle":"2024-05-03T00:08:03.930416Z","shell.execute_reply.started":"2024-05-03T00:08:03.916741Z","shell.execute_reply":"2024-05-03T00:08:03.928866Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[[0.0 1.0 0.0 30.0 54000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.932040Z","iopub.execute_input":"2024-05-03T00:08:03.932678Z","iopub.status.idle":"2024-05-03T00:08:03.945374Z","shell.execute_reply.started":"2024-05-03T00:08:03.932643Z","shell.execute_reply":"2024-05-03T00:08:03.944151Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[0 1 0 0 1 1 0 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.952292Z","iopub.execute_input":"2024-05-03T00:08:03.953489Z","iopub.status.idle":"2024-05-03T00:08:03.964307Z","shell.execute_reply.started":"2024-05-03T00:08:03.953442Z","shell.execute_reply":"2024-05-03T00:08:03.962868Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[0 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font color='lightskyblue'>7- Feature scaling</font>\nFeature scaling marks the end of the data preprocessing in Machine Learning. It is a method to standardize the independent variables of a dataset within a specific range. In other words, feature scaling limits the range of variables so that you can compare them on common grounds.\n\nAnother reason why feature scaling is applied is that few algorithms like gradient descent converge much faster with feature scaling than without it.","metadata":{}},{"cell_type":"markdown","source":"# <img src=\"https://miro.medium.com/max/876/1*dGXqtJOKa_Tbvt9nL3H7KQ.jpeg\" width=\"50%\"/>\n","metadata":{}},{"cell_type":"markdown","source":"# <font color='pink'>Why feature scalling? </font>\nMost of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.\n\nIf left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n\n# <img src=\"https://miro.medium.com/max/640/1*EyPd0sQxEXtTDSJgu72JNQ.jpeg\" width=\"40%\"/>\n","metadata":{}},{"cell_type":"markdown","source":"# <font color='red'>MinMax Scaler</font>\nMinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution.","metadata":{}},{"cell_type":"markdown","source":"# <img src=\"https://miro.medium.com/max/1838/1*QjZVCDwW1TMFTWF3rubemQ@2x.png\" width=\"50%\"/>\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\nX_train[:, 3:] = mm.fit_transform(X_train[:, 3:])\nX_test[:, 3:] = mm.transform(X_test[:, 3:])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.966296Z","iopub.execute_input":"2024-05-03T00:08:03.966849Z","iopub.status.idle":"2024-05-03T00:08:03.981360Z","shell.execute_reply.started":"2024-05-03T00:08:03.966807Z","shell.execute_reply":"2024-05-03T00:08:03.979186Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(X_train[:, 3:])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:03.983916Z","iopub.execute_input":"2024-05-03T00:08:03.984530Z","iopub.status.idle":"2024-05-03T00:08:03.999970Z","shell.execute_reply.started":"2024-05-03T00:08:03.984479Z","shell.execute_reply":"2024-05-03T00:08:03.997642Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"[[0.5120772946859904 0.11428571428571432]\n [0.5652173913043479 0.45079365079365075]\n [0.7391304347826089 0.6857142857142855]\n [0.4782608695652175 0.37142857142857144]\n [0.0 0.0]\n [0.9130434782608696 0.8857142857142857]\n [1.0 1.0]\n [0.34782608695652173 0.2857142857142856]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_test[:, 3:])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:04.002502Z","iopub.execute_input":"2024-05-03T00:08:04.003641Z","iopub.status.idle":"2024-05-03T00:08:04.018464Z","shell.execute_reply.started":"2024-05-03T00:08:04.003592Z","shell.execute_reply":"2024-05-03T00:08:04.017014Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"[[0.1304347826086958 0.17142857142857149]\n [0.43478260869565233 0.5428571428571427]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font color='red'>Standard Scaler</font>\nStandardScaler follows Standard Normal Distribution (SND). Therefore, it makes mean = 0 and scales the data to unit variance.","metadata":{}},{"cell_type":"markdown","source":"# <img src=\"https://i.stack.imgur.com/PZgJ2.png\" width=\"50%\"/>\n","metadata":{}},{"cell_type":"markdown","source":"# <img src=\"https://journaldev.nyc3.digitaloceanspaces.com/2020/10/Standardization.png\" width=\"50%\"/>\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsta = StandardScaler()\nX_train[:, 3:] = sta.fit_transform(X_train[:, 3:])\nX_test[:, 3:] = sta.transform(X_test[:, 3:])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:04.020648Z","iopub.execute_input":"2024-05-03T00:08:04.021113Z","iopub.status.idle":"2024-05-03T00:08:04.034608Z","shell.execute_reply.started":"2024-05-03T00:08:04.021063Z","shell.execute_reply":"2024-05-03T00:08:04.032810Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print(X_train[:, 3:])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:04.035895Z","iopub.execute_input":"2024-05-03T00:08:04.036403Z","iopub.status.idle":"2024-05-03T00:08:04.049296Z","shell.execute_reply.started":"2024-05-03T00:08:04.036353Z","shell.execute_reply":"2024-05-03T00:08:04.047858Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"[[-0.19159184384578537 -1.0781259408412425]\n [-0.014117293757057581 -0.07013167641635436]\n [0.5667085065333245 0.6335624327104541]\n [-0.3045301939022482 -0.3078661727429788]\n [-1.9018011447007983 -1.4204636155515822]\n [1.1475343068237058 1.2326533634535486]\n [1.4379472069688963 1.5749910381638883]\n [-0.740149544120035 -0.5646194287757338]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_test[:, 3:])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T00:08:04.051969Z","iopub.execute_input":"2024-05-03T00:08:04.052497Z","iopub.status.idle":"2024-05-03T00:08:04.067349Z","shell.execute_reply.started":"2024-05-03T00:08:04.052443Z","shell.execute_reply":"2024-05-03T00:08:04.066290Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"[[-1.4661817944830116 -0.9069571034860727]\n [-0.4497366439748436 0.20564033932252992]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <font color='orchid'>When to Use Feature Scalling? </font>\n\n<font color='red'>k-nearest neighbors   </font> with an Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally.\n\nScaling is critical, while performing <font color='red'>Principal Component Analysis(PCA)</font>. PCA tries to get the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features.\n\nWe can speed up <font color='red'>gradient descent</font> by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n\n<font color='red'>Tree based models</font> are not distance based models and can handle varying ranges of features. Hence, Scaling is not required while modelling trees.\n\nAlgorithms like <font color='red'>Linear Discriminant Analysis(LDA)</font>, <font color='red'>Naive Bayes</font> are by design equipped to handle this and gives weights to the features accordingly. Performing a features scaling in these algorithms may not have much effect.","metadata":{}},{"cell_type":"markdown","source":"# <font color='darkgreen'>Normalization Vs. Standardization</font>\nThe two most discussed scaling methods are Normalization and Standardization. Normalization typically means rescales the values into a range of [0,1]. Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance).","metadata":{}},{"cell_type":"markdown","source":"# <img src=\"https://miro.medium.com/max/744/1*HW7-kYjj6RKwrO-5WTLkDA.png\" width=\"70%\"/>\n","metadata":{}}]}